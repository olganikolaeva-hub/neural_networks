-------------------------------------------------------------------------------------------------------------------------------------
Реккурентные нейронные сети
-------------------------------------------------------------------------------------------------------------------------------------
!!! RNN сжимает всю историю (последовательность) в вектор фиксированного размера !!!
Лекции:
1. Vega Institute MGU: https://www.youtube.com/watch?v=ltKaGxinElA&list=PL5FQSF_Ibjj0Oxzy3vXmVxV2LVphPrgxB&index=2
2. GB: https://gb.ru/lessons/246366

0. Нейронные сети, разобранные нами ранее, относятся к классу feed forward нейронных сетей или
   сетей прямого распространения. Выходной сигнал слоя в этих нейронных сетях передавался
   напрямую в следующий слой. Однако есть задачи, в которых нужно обучать нейронную сеть не на
   единичных экземплярах (наподобие изображений), а на наборах последовательностей, например,
   последовательностей слов.

1. В рекуррентной нейронной сети выходной сигнал внутренних слоёв циркулирует в этих слоях
   некоторое время. При обучении такой нейронной сети, прежние выходные сигналы используются как
   дополнительные input'ы. Можно сказать, что эти дополнительные input'ы конкатенируют с
   "нормальными" input'ами предыдущего слоя.

2. Нейронные сети с подобной архитектурой могут использоваться для любых задач, где осуществляется работа с некоторыми
   последовательностями значений, например, с биржевыми котировками. Разновидности рекуррентных
   нейронных сетей также используются для построения ИИ (подобных тем, что обыграли человека в
   компьютерную игру Dota 2). В отличие от свёрточных нейронных сетей, рекуррентные обычно
   содержат небольшое количество слоёв. Так, рекуррентная нейронная сеть в несколько десятков
   слоёв будет считаться большой.
-------------------------------------------------------------------------------------------------------------------------------------
3. Несмотря на то, что RNN могут хорошо справляться со своими задачами, они не могут работать с
   длинными последовательностями. Эффективно они работают только с последовательностями,
   состоящими из 3-4 элементов. Например, для анализа текста отзывов на предмет того,
   положительный он или нет, этого будет недостаточно. Здесь может понадобиться анализ нескольких
   десятков слов, чтобы сделать корректный вывод. Давайте обсудим, почему обычной RNN не удается
   анализировать длинные последовательности.

4. Итого: рекуррентные нейросети - удобная модель для обработки и генерации последовательных данных:
   - динамика цен на акции
   - динамика действий посетителя веб-сайта
   - динамика погоды
   - предложения - последовательности слов, речь - последовательность звуко (каждое слово кодируют векторным представлением
                                                                             низкой размерности,
                                                                             обычно размерности несколько сотен - word2vec, glove)
   - ввидео - последовательность кадров

--------------------------------------------------------------------------------------------------------------------------------------
5. В каждый момент времени мы наблюдаем вектор признаков X_i:j от момента времени i до момента времени j:
   X_i, X_i+1, ..., X_j
   RNN выдает вектор фиксированной размерности y: y_n = RNN(X_1:n)
   Варьируя n получаем отображения RNN* из последовательности в последовательность.

6. Так как RNN сжимает всю историю X_1:n в вектор фиксированного размера y_n, то его можно подавать как вектор признаков другой модели.
   Т.е. с таким представление последовательности удобно работать в качестве переменной в других классических алгоритмов машинного
   обучения. 

7. Внутри себя нейронная сеть содержит некоторое внутренне состояние, которое и обеспечивает реккурентоность.
   Смысл которого состоит в следующем: пересчитывает по текущим данным и предыдущим данным,
   обновляет его в соответствии с новой поступающей информацией.
   Далее - функция активации к новому состоянию.

8. RNN(x1:n,s0) =y1:n̂
   yi=O(si)
   si=R(si−1,xi)
   Развернутая сеть: s4=R(s3,x4) =R(R(s2,x3),x4)=R(R(R(s1,x2),x3),x4) =R(R(R(R(s0,x1),x2),x3),x4)
   При взятии производных подправляться будут веса. Штрафуем агрегированные средние потери.

9. Реккурентная нейронная сеть использует время, подаваться объекты должны в определенной последовательности.
   В обычном ML последовательность при обучении не важна.

10. Acceptor - выдает итоговый y, задача классификации. Функция потерь вида L(y, y_predicted)

--------------------------------------------------------------------------------------------------------------------------------------

11. Из материалов по свёрточным нейронным сетям нам известна проблема исчезающего градиента. В
   случае с большим количеством слоёв значение градиента при последовательном их обновлении
   становиться всё меньше и может стать настолько маленьким, что не будет в состоянии существенно
   изменить поведение нейронов. В рекуррентных нейронных сетях из-за сигнала, циркулирующего
   внутри слоёв, эта проблема становится ещё острее. Причём градиент может стать не только очень
   маленьким, но и крайне большим.


12. Другими словами: У нас возникает произведение большое количество производных. скрытое состояние - это тоже зависимая переменная,
   которая зависит от предыдущего состояния. Функция активации - сигмоида или тангенс, в середине - производная достаточно большая,
   похожая на сглаженную дельта функцию, похожа на гауссиану, и при этом в краях она нулевая.
   Это все приводит к проблеме исчезающих градиентов. Поэтому наш градиент становится очень маленьким.
   Если в предложении 100 слов, то будет брать производную 100 раз. Так это работает и это некоторая проблема.
   Моделировать память можно иначе.

-------------------------------------------------------------------------------------------------------------------------------------
13. LSTM, GRU - две реализации реккурентных нейронный сетей, это реккурентные нейронные сети, где есть хотя бы 1 реккурентный нейрон
   Свой output нейрон прокидывает в самого себя. Для того, чтобы восстановить временные последовательности, нужно иметь память.
   Мы запоминаем контекст. Все, что мы хотим - это добавить нейросети память. Реккурентные нейронные сети этот концепт реализуют.

-------------------------------------------------------------------------------------------------------------------------------------

14. Конкатенируем матрицу весов, она будет той размерности, какой размерности будет временной ряд, на котором мы хотим обучиться.
   Матрица весов у одного нейрона одна, она обновляется!!!

-------------------------------------------------------------------------------------------------------------------------------------


15. Суть реккурентности в том, что мы в момент времени t обращаемся в состояние нейросети, которое было в t-1.
   Нейрон кидает в самого себя свое состояние, полученное на предыдущем шаге.

16. Реккурентные нейроные сети нужны для последовательностей. У реккурентных нейронные сетей есть механизм памяти, мы подаем разные
   элементы последовательности. CNN работает не так: у нас есть объекты, между ними есть связи, но мы не можем подать сверточной сетке
   один элемент и предсказывать следующий. Это разный подход.

17. Решить проблему исчезающего градиента призвана разновидность RNN под названием LSTM.
    
   Long short-term memory (LSTM) юниты — это блоки, из которых состоят слои одной из разновидностей
   рекуррентной нейронной сети(RNN). RNN, состоящая из LSTM юнитов, иногда называется просто
   LSTM. Обычно LSTM юнит представляет собой ячейку, состоящую из input gate, output gate и forget
   gate. Эти ячейки ответственны за запоминание значений на определённые промежутки времени.

   Каждый из этих элементов можно представить как типичный искусственный нейрон в многослойной
   нейронной сети, они вычисляют активацию (используя функцию активации) как взвешенную сумму. Их
   работа сводиться к регуляции потока значений через блок LSTM, поэтому они и называются воротами
   или затворами (gate).

   Понятие долгой памяти в названии возникло из-за того, что они могут
   запоминать информацию на более длинный период времени, чем обычная RNN. LSTM хорошо
   подходит для классификации процессов и предсказания временных последовательностей
   неизвестного размера и неизвестных промежутков между важными событиями.
  
  !!! С технической точки зрения это достигается за счёт ликвидации проблем, связанных с exploding и vanishinggradient'ами.

18. Во-первых, LSTM ячейка берёт предыдущее состояние памяти Ct-1 и умножает на значение в forget
    gate(f), чтобы определить, присутствует ли состояние памяти Ct. Если forget gate значение равно 0, то
    предыдущее состояние памяти полностью забывается, если же f forget gate значение равно 1, то
    предыдущее значение состояния памяти полностью проходит через ячейку (помните, что f gate даёт
    значение между 0 и 1).

    Ct = Ct-1 * ft

    Новое состояние памяти: 
    Ct = Ct + (It * C`t)

    Теперь вычисляем исходное значение:
    Ht = tanh(Ct)



19. Мы подаем на вход не числовые последовательности, мы подаем на вход их численное представление.

20. количество знаков для того, чтобы закодировать двоичное число равно степени двойки.
   например, для того, чтобы закодировать десятичное число 32 двоичным, нужно 5 ячеек: 01000

21. embeding: численное представление слов, отображение слов в пространстве размерности R
   У каждого вектора, которое кодирует слово, должна быть одинаковая длина

22. у слов, которые редко встречаются, обучающая способность ноль.
   бессмысленные, супер редкие слова не берут для построения.
   размерность признакового пространства зависит от мощности нашего алфавита.

23. pad_sequence - приводим наши последовательности (list) к одной длине.

24. embeding добавляем в качестве первого слоя нейронной сети

25. тополь - (n, k), k - длина одного эмбединга

26. генерация текста - это задача регрессии. предсказать следующее слово - следующий эмбединг - это задача регрессии. 

27. В bag of word внутренне состояние аддитивно учитывает предыдущее внутренне состояние плюс новое входной значение.
    Недостаток: порядок входов не имеет значения, поэтому на практике не применяется.
    Чтобы учесть последовательность, надо усложнить пересчет внутреннего состояния.

    Сеть Эльмана чувствительна к порядку входов.

28. Почему может быть проблема взырвающегося градиента: веса могут свободно меняться в широком диапазоне значений,
    если мы их ограничим, добавив регуляризатор,
    дополнительно будет дифференцировать регуляризатор, который будет притягивать веса с нулю и это притяжение может уменьшить
    величину перескоков в неблагоприятную область.


29. С взрывающимся градиентом легче бороться: регуляризация, обрезать норму градиента по порогу.

30. В чем проблема рекурентной нейронной сети - вектор памяти на каждой итерации перезаписывается снова и снова.
    Поэтому такая сеть может быстро забывать прошлое.
    А нам важно заглядывать глубоко в историю.
    Решение - использовать gate - вентиль, который показывает нам, какую информацию можно забыть и перезаписать, а какую инфу нужно
    помнить.
    Параметры нейросети оптимизируются вместе с параметрами вентеля.
   
